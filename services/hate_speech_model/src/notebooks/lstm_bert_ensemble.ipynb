{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import emoji\n",
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from keras.preprocessing import text as prep_text\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from scipy.stats import rankdata\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was adapted from:\n",
    "\n",
    "https://www.kaggle.com/code/chechir/bert-lstm-rank-blender/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 14\n",
    "DATA_PATH = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/jigsaw_unintended_bias/'\n",
    "WORD_EMBEDDINGS = {\n",
    "    'fasttext': '../word_vectors/crawl-300d-2M.vec',\n",
    "    'glove': '../word_vectors/glove.840B.300d.txt'\n",
    "}\n",
    "TARGET_COLUMN = 'target'\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "NUM_MODELS = 1\n",
    "EMBED_DIM = 300\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220  # max word embeddings per document\n",
    "VOCAB_SIZE = 100000  # total distinct words or features - this limits the words to be embedded\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaTextCleaner:\n",
    "    def __init__(self, text: List[str]):\n",
    "        self.text = text\n",
    "\n",
    "    def translate_unicode_symbols(self, to: str = \"utf-8\"):\n",
    "        self.text = [unicodedata.normalize('NFKD', t).encode(to, 'ignore').decode() for t in self.text]\n",
    "\n",
    "    def remove_emojis(self):\n",
    "        self.text = [emoji.replace_emoji(t, replace=' ') for t in self.text]\n",
    "    \n",
    "    def convert_to_lowercase(self):\n",
    "        self.text = [t.lower() for t in self.text]\n",
    "\n",
    "    def expand_contractions(self):\n",
    "        contractions = {\n",
    "            \"ain't\": \"is not\", \n",
    "            \"aren't\": \"are not\", \n",
    "            \"can't\": \"cannot\", \n",
    "            \"'cause\": \"because\", \n",
    "            \"could've\": \"could have\", \n",
    "            \"couldn't\": \"could not\", \n",
    "            \"didn't\": \"did not\", \n",
    "            \"doesn't\": \"does not\", \n",
    "            \"don't\": \"do not\", \n",
    "            \"hadn't\": \"had not\", \n",
    "            \"hasn't\": \"has not\", \n",
    "            \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he would\", \n",
    "            \"he'd've\": \"he would have\",\n",
    "            \"he'll\": \"he will\", \n",
    "            \"he'll've\": \"he will have\",\n",
    "            \"he's\": \"he is\", \n",
    "            \"how'd\": \"how did\", \n",
    "            \"how'd'y\": \"how do you\", \n",
    "            \"how'll\": \"how will\", \n",
    "            \"how's\": \"how is\", \n",
    "            \"i'd\": \"i would\", \n",
    "            \"i'd've\": \"i would have\", \n",
    "            \"i'll\": \"i will\", \n",
    "            \"i'll've\": \"i will have\", \n",
    "            \"i'm\": \"i am\", \n",
    "            \"i've\": \"i have\", \n",
    "            \"isn't\": \"is not\", \n",
    "            \"it'd\": \"it would\", \n",
    "            \"it'd've\": \"it would have\", \n",
    "            \"it'll\": \"it will\", \n",
    "            \"it'll've\": \"it will have\", \n",
    "            \"it's\": \"it is\", \n",
    "            \"let's\": \"let us\", \n",
    "            \"ma'am\": \"madam\", \n",
    "            \"mayn't\": \"may not\", \n",
    "            \"might've\": \"might have\", \n",
    "            \"mightn't\": \"might not\", \n",
    "            \"mightn't've\": \"might not have\", \n",
    "            \"must've\": \"must have\", \n",
    "            \"mustn't\": \"must not\", \n",
    "            \"mustn't've\": \"must not have\", \n",
    "            \"needn't\": \"need not\", \n",
    "            \"needn't've\": \"need not have\", \n",
    "            \"o'clock\": \"of the clock\", \n",
    "            \"oughtn't\": \"ought not\", \n",
    "            \"oughtn't've\": \"ought not have\", \n",
    "            \"shan't\": \"shall not\", \n",
    "            \"sha'n't\": \"shall not\", \n",
    "            \"shan't've\": \"shall not have\", \n",
    "            \"she'd\": \"she would\", \n",
    "            \"she'd've\": \"she would have\", \n",
    "            \"she'll\": \"she will\", \n",
    "            \"she'll've\": \"she will have\", \n",
    "            \"she's\": \"she is\", \n",
    "            \"should've\": \"should have\", \n",
    "            \"shouldn't\": \"should not\", \n",
    "            \"shouldn't've\": \"should not have\", \n",
    "            \"so've\": \"so have\", \n",
    "            \"so's\": \"so as\", \n",
    "            \"this's\": \"this is\", \n",
    "            \"that'd\": \"that would\", \n",
    "            \"that'd've\": \"that would have\", \n",
    "            \"that's\": \"that is\", \n",
    "            \"there'd\": \"there would\", \n",
    "            \"there'd've\": \"there would have\", \n",
    "            \"there's\": \"there is\", \n",
    "            \"here's\": \"here is\", \n",
    "            \"they'd\": \"they would\", \n",
    "            \"they'd've\": \"they would have\", \n",
    "            \"they'll\": \"they will\", \n",
    "            \"they'll've\": \"they will have\", \n",
    "            \"they're\": \"they are\", \n",
    "            \"they've\": \"they have\", \n",
    "            \"to've\": \"to have\", \n",
    "            \"wasn't\": \"was not\", \n",
    "            \"we'd\": \"we would\", \n",
    "            \"we'd've\": \"we would have\", \n",
    "            \"we'll\": \"we will\", \n",
    "            \"we'll've\": \"we will have\", \n",
    "            \"we're\": \"we are\", \n",
    "            \"we've\": \"we have\", \n",
    "            \"weren't\": \"were not\", \n",
    "            \"what'll\": \"what will\", \n",
    "            \"what'll've\": \"what will have\", \n",
    "            \"what're\": \"what are\", \n",
    "            \"what's\": \"what is\", \n",
    "            \"what've\": \"what have\", \n",
    "            \"when's\": \"when is\", \n",
    "            \"when've\": \"when have\", \n",
    "            \"where'd\": \"where did\", \n",
    "            \"where's\": \"where is\", \n",
    "            \"where've\": \"where have\", \n",
    "            \"who'll\": \"who will\", \n",
    "            \"who'll've\": \"who will have\", \n",
    "            \"who's\": \"who is\", \n",
    "            \"who've\": \"who have\", \n",
    "            \"why's\": \"why is\", \n",
    "            \"why've\": \"why have\", \n",
    "            \"will've\": \"will have\", \n",
    "            \"won't\": \"will not\", \n",
    "            \"won't've\": \"will not have\", \n",
    "            \"would've\": \"would have\", \n",
    "            \"wouldn't\": \"would not\", \n",
    "            \"wouldn't've\": \"would not have\", \n",
    "            \"y'all\": \"you all\", \n",
    "            \"y'all'd\": \"you all would\", \n",
    "            \"y'all'd've\": \"you all would have\", \n",
    "            \"y'all're\": \"you all are\", \n",
    "            \"y'all've\": \"you all have\", \n",
    "            \"you'd\": \"you would\", \n",
    "            \"you'd've\": \"you would have\", \n",
    "            \"you'll\": \"you will\", \n",
    "            \"you'll've\": \"you will have\", \n",
    "            \"you're\": \"you are\", \n",
    "            \"you've\": \"you have\",\n",
    "        }\n",
    "        for i in range(len(self.text)):\n",
    "            for k, v in contractions.items():\n",
    "                self.text[i] = self.text[i].replace(k, v)\n",
    "\n",
    "    def clean_swear_words(self):\n",
    "        swear_words = {\n",
    "            \"sh*t\": \"shit\",\n",
    "            \"s**t\": \"shit\",\n",
    "            \"sh*tty\": \"shitty\",\n",
    "            \"sh**ty\": \"shitty\",\n",
    "            \"sh*tting\": \"shitting\",\n",
    "            \"f*ck\": \"fuck\",\n",
    "            \"fu*k\": \"fuck\",\n",
    "            \"f**k\": \"fuck\",\n",
    "            \"f*cked\": \"fucked\",\n",
    "            \"fu*ked\": \"fucked\",\n",
    "            \"f***ed\": \"fucked\",\n",
    "            \"effing\": \"fucking\",\n",
    "            \"f*****g\": \"fucking\",\n",
    "            \"f***ing\": \"fucking\",\n",
    "            \"f**king\": \"fucking\",\n",
    "            \"p*ssy\": \"pussy\",\n",
    "            \"p***y\": \"pussy\",\n",
    "            \"pu**y\": \"pussy\",\n",
    "            \"p*ss\": \"piss\",\n",
    "            \"b*tch\": \"bitch\",\n",
    "            \"bit*h\": \"bitch\",\n",
    "            \"h*ll\": \"hell\",\n",
    "            \"h**l\": \"hell\",\n",
    "            \"cr*p\": \"crap\",\n",
    "            \"d*mn\": \"damn\",\n",
    "            \"stu*pid\": \"stupid\",\n",
    "            \"st*pid\": \"stupid\",\n",
    "            \"n*gger\": \"nigger\",\n",
    "            \"n***ga\": \"nigger\",\n",
    "            \"f*ggot\": \"faggot\",\n",
    "            \"f*g\": \"faggot\",\n",
    "            \"scr*w\": \"screw\",\n",
    "            \"pr*ck\": \"prick\",\n",
    "            \"g*d\": \"god\",\n",
    "            \"s*x\": \"sex\",\n",
    "            \"a*s\": \"ass\",\n",
    "            \"a$$\": \"ass\",\n",
    "            \"a**hole\": \"asshole\",\n",
    "            \"a***ole\": \"asshole\",\n",
    "            \"a-hole\": \"asshole\",\n",
    "            \"a**\": \"ass\",\n",
    "        }\n",
    "        for i in range(len(self.text)):\n",
    "            for k, v in swear_words.items():\n",
    "                self.text[i] = self.text[i].replace(k, v)\n",
    "\n",
    "    def remove_special_chars(self, special_chars: str = None):\n",
    "        if special_chars is None:\n",
    "            special_chars = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n",
    "        for i in range(len(self.text)):\n",
    "            for sc in special_chars:\n",
    "                self.text[i] = self.text[i].replace(sc, ' ' )\n",
    "\n",
    "    def collapse_extra_whitespace(self):\n",
    "        tab_newline_chars = \"\\n\\r\\t\"\n",
    "        for i in range(len(self.text)):\n",
    "            for sc in tab_newline_chars:\n",
    "                self.text[i] = self.text[i].replace(sc, ' ' )\n",
    "            self.text[i] = re.compile(r'\\s+').sub(\" \", self.text[i]).strip()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mtc = MegaTextCleaner(text=[\"doÃ¤g*&ðŸ§”\", \"cÂ©4tðŸ•\", \"f*ck\", '\\u00C7', \"I can't   believe    \"])\n",
    "mtc.translate_unicode_symbols(to=\"ascii\")\n",
    "mtc.remove_emojis()\n",
    "mtc.convert_to_lowercase()\n",
    "mtc.expand_contractions()\n",
    "mtc.clean_swear_words()\n",
    "mtc.remove_special_chars()\n",
    "mtc.collapse_extra_whitespace()\n",
    "mtc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \"\"\"Ensures experiment will run deterministically for any given seed, even with CUDA\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def preprocess(text: pd.Series):\n",
    "    \"\"\"Cleans and processes text.  Function execution order matters.\"\"\"\n",
    "    text = text.tolist()\n",
    "    mtc = MegaTextCleaner(text=text)\n",
    "    mtc.translate_unicode_symbols(to=\"ascii\")\n",
    "    mtc.remove_emojis()\n",
    "    mtc.convert_to_lowercase()\n",
    "    mtc.expand_contractions()\n",
    "    mtc.clean_swear_words()\n",
    "    mtc.remove_special_chars()\n",
    "    mtc.collapse_extra_whitespace()\n",
    "    return mtc.text\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    Converts a line from the embedding file to a tuple of (word, 32-bit numpy array)\n",
    "\n",
    "    :param word: the first element in each line is the word\n",
    "    :param arr: elements 2-n are the embedding dimensions\n",
    "    \"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path: str):\n",
    "    \"\"\"\n",
    "    Utility function to load word embeddings.  Each word embedding looks like:\n",
    "    word 0.3 0.4 0.5 0.6 ...\n",
    "    This function converts the embeddings to a dictionary of {word: numpy array}\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "\n",
    "def get_word_embeddings(word_index: dict, path: str):\n",
    "    \"\"\"\n",
    "    Maps words fround in the text (word_index) to their corresponding word embeddings from the \n",
    "    pre-trained model loaded from (path).  If any words cannot be found in the pre-trained model, \n",
    "    they are tracked in unknown_words.\n",
    "    \"\"\"\n",
    "    embedding_index = load_embeddings(path)\n",
    "    # create an empty matrix of shape (nbr_words, embed_dim)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "    unknown_words = []\n",
    "    \n",
    "    # map all words from the text to their embeddings, if they exist in the embedding index\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def custom_loss(data, targets):\n",
    "    bce_loss_1 = torch.nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1], targets[:,:1])\n",
    "    bce_loss_2 = torch.nn.BCEWithLogitsLoss()(data[:,1:], targets[:,2:])\n",
    "    return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "\n",
    "class SpatialDropout(torch.nn.Dropout2d):\n",
    "    \"\"\"\n",
    "    Implements the functionality of Keras' SpatialDropout1D.\n",
    "    Randomly drop features, i.e. [[1, 1, 1], [2, 1, 2]] -> [[1, 0, 1], [2, 0, 2]]\n",
    "    Compare this with ordinary dropout that drops by sample, i.e. [[1, 1, 1], [2, 1, 2]] -> [[1, 0, 1], [0, 1, 2]]\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # add a dimension of size 1 at position 2, producing (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # re-order dimensions to (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # re-order dimensions to (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # remove dimension of size 1 at position 2, producing (N, T, K)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, num_aux_targets: int):\n",
    "        \"\"\"Sets up neural network architecture\"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # set up a non-trainable, pre-trained embedding layer from the provided embedding_matrix\n",
    "        self.embedding = torch.nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "        self.embedding.weight = torch.nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)  # randomly drop this percent of features\n",
    "        \n",
    "        # each bidirectional layer outputs 2 sequences: 1 forward, 1 backward, and concatenates them\n",
    "        # so stacking 2 enriches the sequence features\n",
    "        self.lstm1 = torch.nn.LSTM(\n",
    "            input_size=EMBED_DIM, \n",
    "            hidden_size=LSTM_UNITS, \n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm2 = torch.nn.LSTM(\n",
    "            input_size=LSTM_UNITS * 2, \n",
    "            hidden_size=LSTM_UNITS, \n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "        # skip connections...\n",
    "        # add a product of a dense layer with the hidden layer to the output of the the hidden layer\n",
    "        self.linear1 = torch.nn.Linear(in_features=DENSE_HIDDEN_UNITS, out_features=DENSE_HIDDEN_UNITS, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(in_features=DENSE_HIDDEN_UNITS, out_features=DENSE_HIDDEN_UNITS, bias=True)\n",
    "        \n",
    "        self.linear_out = torch.nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        \n",
    "        # auxiliary outputs to be predicted as an alternative to the main output\n",
    "        self.linear_aux_out = torch.nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Implements forward pass\"\"\"\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "\n",
    "        avg_pool = torch.mean(h_lstm2, 1)  # global mean pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)  # global max pooling\n",
    "\n",
    "        # concatenate to reshape from (batch_size, MAX_LEN, LSTM_UNITS * 2) to h_conc (BATCH_SIZE, LSTM_UNITS * 4)\n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train, \n",
    "    test, \n",
    "    loss_fn, \n",
    "    output_dim: int, \n",
    "    lr: float = 0.001,\n",
    "    batch_size: int = 512, \n",
    "    n_epochs: int = 4, \n",
    "    enable_checkpoint_ensemble: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model on the training set.  \n",
    "    \"\"\"\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    # decay the learning rate using a schedule of 0.6^epoch\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # increment learning rate schedule\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            # the target is the final column in data\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            # forward pass and calculate loss\n",
    "            y_pred = model(*x_batch)            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # zero out the gradients for the optimizer, now that the loss has been calculated\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights using the optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # track the mean loss over all batches in this epoch\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        # run each batch of the test data through the model\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        # append the batch test_preds to the epoch's all_test_preds\n",
    "        all_test_preds.append(test_preds)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs} \\t loss={avg_loss:.4f} \\t time={elapsed_time:.2f}s')\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(predictions, weights, type_=\"linear\"):\n",
    "    \"\"\"\n",
    "    Combines predictions from several models.\n",
    "    \n",
    "    :param weights: sample weights\n",
    "    \"\"\"\n",
    "    assert np.isclose(np.sum(weights), 1.0)  # weights should be close to 1.0\n",
    "    \n",
    "    # linear blend by taking the average between the models, weighted by sample\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "\n",
    "    # harmonic blend by taking the average of the reciprocals (handles outliers better than straight avg)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "\n",
    "    # geometric blend handles discrepancies between the scales of the preds between the models\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "\n",
    "    # ranks the predictions from the models and takes the mean rank, weighted by sample\n",
    "    elif type_ == \"rank\":\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect ~2 Gb RAM for the data\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "\n",
    "x_train = preprocess(train_df['comment_text'])\n",
    "# y_train to be defined after weighting the samples\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = preprocess(test_df['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 (1804874,)\n",
      "\n",
      " () 3.209226860170181 3.209226860170181\n"
     ]
    }
   ],
   "source": [
    "# weight the training samples\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "print(weights.mean(), weights.shape)\n",
    "\n",
    "# sum the binary identity columns to give rows with identity values more weight (they contain more information)\n",
    "weights += (train_df[IDENTITY_COLUMNS].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "# when the target is 1, increase the weights further by counting the identity columns with 0 values\n",
    "# this increases the weights for the positive class (toxic comment)\n",
    "weights += (\n",
    "    (\n",
    "        (train_df['target'].values >= 0.5).astype(bool).astype(int) \n",
    "        + (train_df[IDENTITY_COLUMNS].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(int) \n",
    "    ) > 1\n",
    ").astype(bool).astype(int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "# when the target is 0, increase the weights by counting the identity columns with value of 1\n",
    "# multiply by some constant (5) to give this weighting more impact\n",
    "# this increases the weights for the negative class (not a toxic comment)\n",
    "weights += (\n",
    "    (\n",
    "        (train_df['target'].values < 0.5).astype(bool).astype(int) \n",
    "        + (train_df[IDENTITY_COLUMNS].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(int) \n",
    "    ) > 1 \n",
    ").astype(bool).astype(int) / 4\n",
    "\n",
    "# normalize the weights\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "print(\"\\n\", loss_weight.shape, loss_weight.min(), loss_weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1804874, 8])\n"
     ]
    }
   ],
   "source": [
    "y_train = np.vstack([(train_df[TARGET_COLUMN].values >= 0.5).astype(int), weights]).T\n",
    "y_train = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features (Vocab Size) 100000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = prep_text.Tokenizer(num_words=VOCAB_SIZE, filters='', lower=True)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "max_features = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print(f\"Max Features (Vocab Size) {max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [01:01, 32658.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (fast text):  165824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:07, 32570.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (glove):  162077\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings\n",
    "\n",
    "fasttext_embeddings, fasttext_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['fasttext'])\n",
    "print('Unknown words (fast text): ', len(fasttext_unknown_words))\n",
    "\n",
    "glove_embeddings, glove_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['glove'])\n",
    "print('Unknown words (glove): ', len(glove_unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape:  (318636, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([fasttext_embeddings, glove_embeddings], axis=-1)\n",
    "print(\"Embedding matrix shape: \", embedding_matrix.shape)\n",
    "\n",
    "del fasttext_embeddings\n",
    "del glove_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data to CUDA\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long)#.cuda()\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.long)#.cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32)#.cuda()\n",
    "\n",
    "# convert to tensor datasets\n",
    "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = data.TensorDataset(x_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = []\n",
    "\n",
    "# train NUM_MODELS models and average their output for the final predictions\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('\\nModel ', model_idx)\n",
    "\n",
    "    # fit each model with a different seed, otherwise they will be identical\n",
    "    seed_everything(SEED + model_idx)\n",
    "    \n",
    "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "    #model.cuda()\n",
    "    \n",
    "    test_preds = train_model(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        test_dataset, \n",
    "        output_dim=y_train_torch.shape[-1],\n",
    "        loss_fn=nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    )\n",
    "    all_test_preds.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lstm = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset vs DataLoader vs DataCollator\n",
    "\n",
    "#### Dataset\n",
    "A tensor dataset is similar to a Pandas dataframe.  It just stores the data.\n",
    "\n",
    "#### DataLoader\n",
    "A Pytorch iterator over a tensor dataset.  This is used to form batches.  It has an argument, collate_fn, that can be used for on-the-fly transformations of the batches.\n",
    "\n",
    "#### DataCollator\n",
    "A HuggingFace class for forming batches and performing on-the-fly padding/truncation of the batches (or other transformations).  Under the hood, HuggingFace passes the DataCollator to Pytorch's DataLoader as the collate_fn argument. If you perform padding/truncation on the data before creating a tensor dataset, then you do not need a DataCollator, although a DataCollator can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes text to uniform shape using padding and truncation.  \n",
    "    This function (and the DataLoader) could be replaced by a DataCollator object that would \n",
    "    do everything on the fly, rather than ahead of time.\n",
    "    \"\"\"\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    nbr_texts_longer_than_max_seq_length = 0\n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        # restrict tokenized text to max_seq_length\n",
    "        if len(tokens_a) > max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            nbr_texts_longer_than_max_seq_length += 1\n",
    "        # insert CLS and SEP tokens around the tokenized text, and zero pad if the length < max_seq_length\n",
    "        one_token = tokenizer.convert_tokens_to_ids(\n",
    "            [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        ) + [0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:18<00:00, 5399.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# BERT is pre-trained so it is ready to make predictions for the test set\n",
    "# the test set must be tokenized first...\n",
    "X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_LEN, tokenizer)\n",
    "\n",
    "test_preds = np.zeros((len(X_test)))\n",
    "\n",
    "# convert the tokenized test set to a TensorDataset (the kind of dataset HuggingFace likes)\n",
    "X_test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test))\n",
    "\n",
    "# now pass the dataset to a DataLoader (an iterator wrapper for a TensorDataset), which will load it by batch\n",
    "X_test_dataloader = torch.utils.data.DataLoader(X_test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e6806ae50eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         )\n\u001b[0;32m-> 1017\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    604\u001b[0m                 )\n\u001b[1;32m    605\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    607\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "all_logits = np.empty([0, 2])\n",
    "for batch in X_test_dataloader:\n",
    "    \n",
    "    input_ids = tuple(b.to(device) for b in batch)[0]\n",
    "    attention_masks = tuple((b > 0).to(device) for b in batch)[0]  # False where zero-padded\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(input_ids, attention_masks)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    all_logits = np.vstack([all_logits, torch.softmax(logits, dim=1).detach().cpu().numpy()])\n",
    "    # all_logits = torch.sigmoid(torch.tensor(all_logits)).numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bert = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': all_logits\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.333, 0.667]\n",
    "submission[\"prediction\"] = ensemble_predictions(\n",
    "    [submission_bert.prediction.values, submission_lstm.prediction.values],\n",
    "    weights,\n",
    "    type_=\"rank\",\n",
    ")\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
